---
title: Some thoughts on justifications for filtering
author: Aaron Lun
date: 4 March 2017 
output:
  html_document:
    fig_caption: false 
---

```{r, echo=FALSE, results="hide"}
dir.create("figure-cor", showWarning=FALSE)
knitr::opts_chunk$set(error=FALSE, warning=FALSE, message=FALSE, fig.path="figure-cor/")
options(width=100)
```

# To improve HVG detection

## Overview

The idea here is that low-abundance genes cannot achieve large variances because they have non-negativity constraints.
If a low-abundance gene can never be detected as a HVG, regardless of the distribution of its counts, it should be removed beforehand.
This reduces the severity of the multiple testing correction and improves power for other genes that have a chance.
The following arguments are based on the mean filter, though the "at least n" filter is also briefly discussed.

## Based on the upper bound for the variance of log-expression

At a given mean count, the variance of log-counts is bounded as the counts must be non-negative.
A gene with a low mean count can only achieve a particular variance (of the log-counts).
Below, we use linear programming to identify the maximum variance of logs at any specified mean of logs/mean of counts, ranging from 0.02 to 2 for both values.
We optimize over the proportions of cells with counts from 0 to 200.

```{r}
library(lpSolve)
counts <- 0:200
lmeanfun <- log2(counts+1)
meanfun <- counts

sumfun <- rep(1, length(counts))
sumval <- 1
posfun <- diag(length(counts))

collected <- list()
for (meanval in 1:100/50) {
    for (lmeanval in 1:100/50) {
        lvarfun <- (log2(counts+1) - lmeanval)^2
        out <- lp(direction="max", objective.in=lvarfun,
        const.mat=rbind(meanfun, lmeanfun, sumfun, posfun),
        const.dir=c("=", "=", "=", rep(">=", length(counts))),
        const.rhs=c(meanval, lmeanval, sumval, rep(0, length(counts))))
        if (out$objval==0) { next }
        collected[[length(collected)+1]] <- c(lmeanval, meanval, out$objval)
    }
}
collected <- do.call(rbind, collected)
```

We can use this to construct a plot showing, at every mean of counts, the maximum variance achievable as a function of the log-mean.
One can overlay the technical noise trend on top of this plot to justify the filter threshold.
If the technical trend lies above the curve of maximal achievable variance at a given threshold, then the filtered genes would never be called as HVGs.
This justifies the choice of filter for that analysis.

```{r}
plot(0, 0, type="n", xlim=c(0, 2), ylim=c(0, 2),xlab="Log-mean", ylab="Max variance")
displayvals <- c(0.02, 0.05, 0.1, 0.2, 0.5, 1, 2)
cols <- topo.colors(length(displayvals))
for (i in seq_along(displayvals)) {
    meanval <- displayvals[i]
    to.use <- collected[collected[,2] <= meanval,,drop=FALSE]
    to.use <- to.use[order(to.use[,1]),,drop=FALSE]
    keep <- cummax(to.use[,3]) <= to.use[,3] | rev(cummax(rev(to.use[,3]))) <= to.use[,3]
    to.use <- to.use[keep,,drop=FALSE]
    lines(to.use[,1], to.use[,3], col=cols[i], lwd=2)
    text(tail(to.use[,1], 1), tail(to.use[,3], 1), pos=4, meanval, col=cols[i])
}
```

In the HSC data set (result of running https://github.com/MarioniLab/BiocWorkflow2016/workflow.Rmd), an average count of 1 corresponds to a maximum variance of around 1.
The technical noise is around about this much anyway, so it makes no sense to keep genes with lower average counts, because they just wouldn't be called as HVGs.
In fact, we could be more stringent and require some minimum increase above the technical noise, but we won't do that.

```{r}
to.use <- collected[collected[,2] <= 1,,drop=FALSE]
plot(to.use[,1], to.use[,3], xlim=c(0, 1), ylim=c(0, 1.5))
comp <- read.table("hsc_hvg.tsv", header=TRUE)
o <- order(comp$mean)
lines(comp$mean[o], comp$tech[o], col="red", pch=16, cex=0.5)
```

Similar logic applies to the analysis of the brain data set from the same workflow.
Here, the technical noise is lower due to the use of UMIs, so we set a smaller filter.
It depends somewhat on whether you want to extrapolate the technical trend with `rule=2` or as a line passing through the origin.
In both cases, though, it's safe to say that the maximum variances are less than or comparable to the trend.

```{r}
to.use <- collected[collected[,2] <= 0.1,,drop=FALSE]
plot(to.use[,1], to.use[,3],xlim=c(0, 1), ylim=c(0, 1))
comp <- read.table("brain_hvg.tsv", header=TRUE)
o <- order(comp$mean)
lines(comp$mean[o], comp$tech[o], col="red", pch=16, cex=0.5)
```

Note that directly filtering on the log-means is _technically_ correct, in that it's independent of HVG detection.
That probably doesn't matter much, though, given that we filter on the raw means in `voom` prior to _limma_ anyway.

## Based on the upper bound for the coefficient of variation

A similar problem applies for methods based on the squared coefficient of variation (CV^2^).
For a given number of cells, there is an upper bound on the CV^2^ that can be achieved.
However, the trend is fitted such that it is usually linear with decreasing mean.
This means that, when extrapolating from the spike-ins to low-abundance genes, the fitted value of the trend will pass the cap.
It is subsequently impossible to detect genes with lower means as HVGs, so these should be filtered out beforehand.
The critical point described above is dependent on the shape of the technical trend.
This may be more or less stringent than the threshold for the variance of log-expression, it's hard to tell in general.

## Based on the Poisson limit

If you have a gene where the maximum count is 1, you can treat this as a Bernoulli random variable.
This has variance that is always sub-Poisson, no matter how heterogeneous the true gene expression is across cells.
HVG calling becomes conceptually pointless for these genes, given that capture (and sequencing, for non-UMI methods) would be at least Poisson anyway.
In fact, the mean-variance trend ceases to have any scatter because all genes will fall on the $V = \mu^{-1} - 1$ line.
Filtering gets rid of these low-abundance genes that are not going to be called anyway.
(The exact threshold depends on the number of cells and the true dispersion; fewer cells and lower dispersions makes a Bernoulli RV more likely.)

## Using the "at least N" filter

This filter selects genes that are "expressed" in at least _N_ cells.
However, the lost genes are not guaranteed to fail HVG detection. 
It is possible to get arbitrarily significant HVGs with either detection method if you pump up the counts in _N-1_ cells.
Conversely, the genes expressed below the mean filter are constrained in how significant they can be (see above).
The choice between the two, then, seems obvious if you want to detect HVGs (i.e., "uninteresting" genes are those driven by technical noise).

# To protect the normalization machinery

Low counts are problematic for median-based normalization, as the count:mean ratios are less precise and the median is not accurate estimator of the mean.
Consider the following simulation, where we compute the median for 10000 low-abundance genes, and calculate its expectation and variance.

```{r}
set.seed(100)
ngenes <- 10000
niter <- 100
vals <- list()
for (it in seq_len(20)) {
    lambda <- it/4
    y <- matrix(rpois(ngenes*niter, lambda=lambda), ncol=ngenes)
    rat <- y/rowMeans(y)
    meds <- apply(rat, 1, median)
    vals[[it]] <- c(lambda, mean(meds), sd(meds))
}
vals <- do.call(rbind, vals)
```

You can see that at counts below 2, the estimation error from using the median is pretty bad (exceeding 20% from the expected value of unity).
It's at a similar point that the variance of the estimator also increases, though the bias is the major contributor here,
Obviously you can expect that this will get even worse for overdispersed data where both the variance and bias are likely to increase.

```{r, fig.width=10, fig.height=6}
par(mfrow=c(1,2))
plot(vals[,1], vals[,2], xlab="Lambda", ylab="Expected median ratio")
plot(vals[,1], vals[,3], xlab="Lambda", ylab="Standard deviation")
```

In the deconvolution method, you can consider the threshold multiplied by the minimum pool size (20 cells by default) as the minimum pooled count.
This gives us pooled counts of around `20*1` for read data, which should be large enough to avoid the above problems for NB dispersions below 0.5.
For UMI data, we get pooled counts of `20*0.1` -- however, the variability is near-Poisson anyway, so variability and bias in the ratio should be okay.
Any biases are probably tolerable if the data set is dominated by genes in the the high-abundance peak.
(They may also cancel out to some extent, given that the bias fluctuates in the plot above).


